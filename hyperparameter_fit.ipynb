{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76f37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import tree \n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "import xgboost as xgb_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7610cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualise\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c525e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files\n",
    "training = pd.read_csv(\"training.csv\", sep= \";\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca22eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what to drop\n",
    "to_drop = [\"neighbour code\", \"target code\", \"predictive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a7b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers\n",
    "xgb = xgb_package.XGBClassifier(objective=\"binary:logistic\", random_state=42, use_label_encoder=False)\n",
    "rfc = RandomForestClassifier(n_estimators=1000, min_samples_leaf=3, max_depth= 10, max_samples=33,\n",
    "                            bootstrap= True, random_state= 42,\n",
    "                            class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e25df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "def prepare_scale_data(training=training, to_drop= to_drop):\n",
    "    \"\"\"\n",
    "    This function prepares the training data for the machine learning.\n",
    "    Drops the previously selected columns using the to_drop list without renaming the remaining columns.\n",
    "    Splits the training data to input parameters and labels.\n",
    "    Lists the column of input parameters.\n",
    "    Uses the StandardScaler to scale the training data.\n",
    "\n",
    "    Args:\n",
    "        to_drop: The list of column names which are going to be dropped from the training and unknown dataframe. \n",
    "        training: The dataframe of input parameters which are intended to use to the training of the machine learning model.\n",
    "        \n",
    "    Returns:\n",
    "        X: The input data from the training dataframe.\n",
    "        y: The labels for the training dataframe.\n",
    "        feature_names: The list of column names in the X dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    training_deleted = training.drop(columns= to_drop)\n",
    "    X = training_deleted.iloc[:,:-1]\n",
    "    y = training_deleted.iloc[:, -1]\n",
    "    feature_names = list(X.columns)\n",
    "    \n",
    "    #skála igazítása\n",
    "    scale = StandardScaler()\n",
    "    scale.fit(X)\n",
    "    X = scale.transform(X)\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a826e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare all data\n",
    "X, y, feature_names = prepare_scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686e20fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 1\n",
      "max_resources_: 100\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 100\n",
      "n_resources: 1\n",
      "Fitting 50 folds for each of 100 candidates, totalling 5000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 34\n",
      "n_resources: 3\n",
      "Fitting 50 folds for each of 34 candidates, totalling 1700 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 12\n",
      "n_resources: 9\n",
      "Fitting 50 folds for each of 12 candidates, totalling 600 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 27\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 2\n",
      "n_resources: 81\n",
      "Fitting 50 folds for each of 2 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'warm_start': False,\n",
       " 'oob_score': False,\n",
       " 'n_jobs': 31,\n",
       " 'min_weight_fraction_leaf': 0,\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 3,\n",
       " 'max_samples': 61,\n",
       " 'max_leaf_nodes': 27,\n",
       " 'max_features': None,\n",
       " 'max_depth': 5,\n",
       " 'criterion': 'log_loss',\n",
       " 'class_weight': 'balanced',\n",
       " 'ccp_alpha': 0,\n",
       " 'n_estimators': 81}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RF\n",
    "clf = RandomForestClassifier(random_state= 42)\n",
    "np.random.seed(0)\n",
    "\n",
    "param_distributions = {\"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "                      \"max_depth\": range(1,15+1, 2),\n",
    "                      \"min_samples_split\": range(2,10+1, 2),\n",
    "                      \"min_samples_leaf\": range(1,10+1, 2),\n",
    "                      \"min_weight_fraction_leaf\": [0.5, 0.4, 0.3, 0.2, 0.1, 0],\n",
    "                      \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "                      \"max_leaf_nodes\": range(2,30+1),\n",
    "                      \"oob_score\": [True, False],\n",
    "                      \"n_jobs\": range(1,50+1, 10),\n",
    "                      \"warm_start\": [True, False],\n",
    "                      \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "                      \"ccp_alpha\": [0.5, 0.4, 0.3, 0.2, 0.1, 0],\n",
    "                      \"max_samples\": range(1,72+1, 10)}\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, param_distributions,\n",
    "                               resource='n_estimators',\n",
    "                               max_resources= 100,\n",
    "                               random_state= 42,\n",
    "                               cv= 50,\n",
    "                               verbose= 3,\n",
    "                               n_jobs= 30).fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebedad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 5\n",
      "n_required_iterations: 5\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 1\n",
      "max_resources_: 100\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 100\n",
      "n_resources: 1\n",
      "Fitting 50 folds for each of 100 candidates, totalling 5000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 34\n",
      "n_resources: 3\n",
      "Fitting 50 folds for each of 34 candidates, totalling 1700 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 12\n",
      "n_resources: 9\n",
      "Fitting 50 folds for each of 12 candidates, totalling 600 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 4\n",
      "n_resources: 27\n",
      "Fitting 50 folds for each of 4 candidates, totalling 200 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 2\n",
      "n_resources: 81\n",
      "Fitting 50 folds for each of 2 candidates, totalling 100 fits\n",
      "[14:27:18] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.5,\n",
       " 'scale_pos_weight': 0.75,\n",
       " 'reg_lambda': 0.25,\n",
       " 'reg_alpha': 0.5,\n",
       " 'min_child_weight': 0.75,\n",
       " 'max_leaves': 11,\n",
       " 'max_depth': 7,\n",
       " 'max_delta_step': 0,\n",
       " 'max_bin': 38,\n",
       " 'learning_rate': 1,\n",
       " 'importance_type': 'cover',\n",
       " 'grow_policy': 'lossguide',\n",
       " 'gamma': 0,\n",
       " 'criterion': 'log_loss',\n",
       " 'booster': 'dart',\n",
       " 'n_estimators': 81}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGB\n",
    "clf = xgb_package.XGBClassifier(random_state= 42)\n",
    "np.random.seed(0)\n",
    "\n",
    "param_distributions = {\"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "                      \"max_depth\": range(1,10+1, 2),\n",
    "                      \"max_leaves\": range(1,20+1, 2),\n",
    "                      \"max_bin\": range(2,40+1, 2),\n",
    "                      \"grow_policy\": [\"depthwise\", \"lossguide\"],\n",
    "                      \"learning_rate\": [1, 0.5, 0],\n",
    "                      \"booster\": [\"gbtree\", \"gblinear\", \"dart\", None],\n",
    "                      \"gamma\": [1, 0.75, 0.5, 0],\n",
    "                      \"min_child_weight\": [0.75, 0.5, 0.25],\n",
    "                      \"max_delta_step\": [1, 0.5, 0],\n",
    "                      \"subsample\": [0.75, 0.5, 0],\n",
    "                      \"reg_alpha\": [0.5, 0.25, 0],\n",
    "                      \"reg_lambda\": [0.5, 0.25, 0],\n",
    "                      \"scale_pos_weight\": [0.75, 0.5, 0.25],\n",
    "                      \"importance_type\": [\"gain\", \"weight\", \"cover\", \"total_gain\", \"total_cover\"]}\n",
    "\n",
    "search = HalvingRandomSearchCV(clf, param_distributions,\n",
    "                               resource='n_estimators',\n",
    "                               max_resources= 100,\n",
    "                               random_state= 42,\n",
    "                               cv= 50,\n",
    "                               verbose= 3,\n",
    "                               n_jobs= 40).fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb746a2b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
